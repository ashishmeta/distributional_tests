{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSS - Extracting Topics from Text ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Super simple script (SSS) for summarising stuff*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) is one of the most commonly used methods for extracting groups of topics from text. Latent Semantic Indexing is another possibility. There are good posts on LDA and LSI on the interwebs, so I shall not go into the math or theory of these methods. Rather, I shall just demonstrate what the LDA and LSI is good for - discovering groups of topics in body of text (or what most term as the corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An aside - this site provides a good introduction to LDA - http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/; and there are a number of threads on quora on the difference between LDA and LSI, such as this - \n",
    "https://www.quora.com/Whats-the-difference-between-Latent-Semantic-Indexing-LSI-and-Latent-Dirichlet-Allocation-LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to find something interesting to run LDA/LSI on. Ideally, the larger the more interesting. But as this is just a demo, we will use 'Ulysses' from the Project Gutenberg site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = 'https://www.gutenberg.org/files/4300/4300-0.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data with the 'requests' library as per what we have done for the other scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "corpus = requests.get(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a combination of string functions and the 're' library, we strip out some of the special characters (e.g. newlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  The Project Gutenberg EBook of Ulysses, by James Joyce    This eBook is for the use of anyone anywhere at no cost and with almost  no restrictions whatsoever. You may copy it, give it away or re-use  it under the terms of the Project Gutenberg License included with this  eBook or online at www.gutenberg.org      Title: Ulysses    Author: James Joyce    Release Date: August 1, 2008 [EBook #4300]  Last Updated: August 17, 2017    Language: English    Character set encoding: UTF-8    *** START OF THIS PROJECT GUTENBERG EBOOK ULYSSES ***          Produced by Col Choat, and David Widger.            Ulysses    by James Joyce          — I —          [ 1 ]    Stately, plump Buck Mulligan came from the stairhead, bearing a bowl of  lather on which a mirror and a razor lay crossed. A yellow dressinggown,  ungirdled, was sustained gently behind him on the mild morning air. He  held the bowl aloft and intoned:    —Introibo ad altare Dei.    Halted, he peered down the dark winding stairs and call'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_text = corpus.text.strip('\\ufeff')\n",
    "regex = re.compile(r'[\\n\\r\\t]')\n",
    "corpus_text=regex.sub(' ', corpus_text)\n",
    "corpus_text = corpus_text.replace('\\\\', '')\n",
    "corpus_text[:1000] # First 1000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will use gensim to do LDA and LSI, so let's import the libraries first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/pydata/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_tokens is just a simple function to process the text we downloaded, and strip out the stopwords (things like is, are ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    return [t for t in simple_preprocess(text) if t not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = get_tokens(corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132581"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 25 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'ulysses',\n",
       " 'james',\n",
       " 'joyce',\n",
       " 'ebook',\n",
       " 'use',\n",
       " 'cost',\n",
       " 'restrictions',\n",
       " 'whatsoever',\n",
       " 'copy',\n",
       " 'away',\n",
       " 'use',\n",
       " 'terms',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'license',\n",
       " 'included',\n",
       " 'ebook',\n",
       " 'online',\n",
       " 'www',\n",
       " 'gutenberg',\n",
       " 'org',\n",
       " 'title']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then convert into a basket of words - basically pre-preparing the text so that we can feed it into the the LDA/LSI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary([tokens])\n",
    "corpus = [dictionary.doc2bow(token) for token in [tokens]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to build the LDA model for this text. The main parameters are:\n",
    "- num_topics - number of topic groups we want\n",
    "- update_every - 0 for once-off; 1 for online learning (i.e. we will feed new data and update model)\n",
    "- passes - number of passes through the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaModel(corpus, \n",
    "                                   num_topics=5, \n",
    "                                   id2word=dictionary, \n",
    "                                   update_every=0,\n",
    "                                   passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.000*\"bloom\" + 0.000*\"said\" + 0.000*\"mr\" + 0.000*\"like\" + 0.000*\"old\" + 0.000*\"stephen\" + 0.000*\"time\" + 0.000*\"says\" + 0.000*\"man\" + 0.000*\"eyes\"'),\n",
       " (1,\n",
       "  '0.000*\"said\" + 0.000*\"bloom\" + 0.000*\"like\" + 0.000*\"mr\" + 0.000*\"says\" + 0.000*\"stephen\" + 0.000*\"man\" + 0.000*\"old\" + 0.000*\"yes\" + 0.000*\"eyes\"'),\n",
       " (2,\n",
       "  '0.000*\"said\" + 0.000*\"like\" + 0.000*\"bloom\" + 0.000*\"mr\" + 0.000*\"stephen\" + 0.000*\"old\" + 0.000*\"says\" + 0.000*\"man\" + 0.000*\"yes\" + 0.000*\"hand\"'),\n",
       " (3,\n",
       "  '0.000*\"said\" + 0.000*\"bloom\" + 0.000*\"like\" + 0.000*\"mr\" + 0.000*\"stephen\" + 0.000*\"old\" + 0.000*\"know\" + 0.000*\"yes\" + 0.000*\"says\" + 0.000*\"man\"'),\n",
       " (4,\n",
       "  '0.009*\"said\" + 0.007*\"bloom\" + 0.005*\"like\" + 0.005*\"mr\" + 0.004*\"stephen\" + 0.004*\"old\" + 0.003*\"says\" + 0.003*\"man\" + 0.003*\"time\" + 0.003*\"yes\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi_model = gensim.models.LsiModel(corpus, \n",
    "                                   id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.376*\"said\" + 0.311*\"bloom\" + 0.227*\"like\" + 0.224*\"mr\" + 0.178*\"stephen\" + 0.153*\"old\" + 0.147*\"says\" + 0.140*\"man\" + 0.118*\"time\" + 0.112*\"yes\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics generated off such a small text body is obviously not satisfactory, but the simple script shows how simple it would be to do this for any other collection of text. \n",
    "\n",
    "We must however understand that these methods can only show us the collection of words that are close to each other and can be grouped as topics. We will still have to eyeball to see if they make sense, and how we would link the words in each of these clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydata]",
   "language": "python",
   "name": "conda-env-pydata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
